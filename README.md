## Future Computing Technology Assessment

Technology assessment initiatives at SURFsara within [**SURF Open Innovation Lab**](https://www.surf.nl/en/the-surf-cooperative/surf-open-innovation-lab) has a clear goal of understanding the performance of numerous HPC and AI workloads on upcoming computing technologies with a focus on performance analysis, collaboration and open sharing of results. 

As a HPC center, we help researchers to solve challenges associated with their applications on our infrastructure. In this particular initiative, we focus on evaluating, assessing, and benchmarking modern and emerging computing architectures for several application domains such as high performance computing, machine Learning and data visualisation. We have identified four major dimensions of effort. 

1. Consistent, reproducible and open benchmarking.
2. Access to the latest compute resources for external and internal users.  
3. External collaborations and the development of innovative HPC services.
3. Knowledge dissemination through workshops and seminars. 


### Open Benchmarking

Here the focus is on aggregrating several relevant benchmarks and standardised compilation, execution and results extraction in a common framework. The idea is to spend less time organising benchmarks and more time in designing new tests, execution and qualitative analysis of new computing systems making the process of benchmarking itself more reproducible, open and community engaging.
	
We are using [**Reframe**](https://github.com/eth-cscs/reframe), a HPC regression testing framework from [**CSCS**](https://www.cscs.ch) to automate the process of benchmarking new systems and compute architectures. We are working on developing core tests in [**Reframe**](https://github.com/eth-cscs/reframe) and would be making it open source soon. 

The tests pipeline would involve minimal software installation on the remote system and flexible integration of new tests and benchmarks inline with **SURF's** HPC infrastructure. We have been using **Reframe** to test experimental configuration located inside **SURFsara** and **University of Amsterdam** and would also be using it to test **DAS-6** systems. 

Our team here at **SURFsara** would be enthusiastic to include different benchmarks covering different scientific disciplines and domains. 
	
<Diagram to explain>

### Access to experimental compute resources

With the availability of numerous computing architectures it becomes important to make scientific application as portable as possible. We would like to facilitate researchers across different spectrum of science and engineering to access experimental compute resources and peform comparative performance analysis to better understand their computational workloads for uncharted computing systems. 

At the moment we host following experimental systems at **SURFsara**

1. 2x ARM 64 core from Huawei. 
2. 1x Intel Gold 6128 12c + 1x Nvidia GPU RTX 2080 + 1x U250 Xilinx FPGA. 
3. 2x AMD EPYC Naples 32C
4. 1x AMD EPYC Naples 32c + 4x AMD GPU MI50

### External Collaborations



### Seminars and Workshops

We did organise several workshops, seminars and external talks and some of them are listed as follows : 

1. [Building Affordable and Programmable Exascale Capable Supercomputers] (https://www.linkedin.com/pulse/building-affordable-programmable-exascale-capable-sagar-dolas/)
2. [Technology Assessment overview](https://www.youtube.com/watch?v=nR4Z0TsROZc&feature=youtu.be)
3. [GPU conference NLeSC](http://fmttools.ewi.utwente.nl/NIRICT_GPGPU/events.html)
4. [Micro-benchmarking and Performance analysis of Skylake Silver nodes deployed in the  National LISA cluster](https://docs.google.com/presentation/d/1VbI8bHLwITwqDSOA1URfMHn7kZw2Gx94hg4UnSVIYPE/edit?usp=sharing)
5. Guest Lunch lecture on "Hardware based numerics on distributed hardware" from TU Delft at **University of Amsterdam** (*Slides availalble on-demand*)

We would like to organise more of these workshops in future with intention of diffusing as much as information as possible. 


