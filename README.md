## Future Computing Technology Assessment

Technology assessment initiatives at SURFsara within [**SURF Open Innovation Lab**](https://www.surf.nl/en/the-surf-cooperative/surf-open-innovation-lab) has a clear goal of understanding the performance of numerous HPC and AI workloads on upcoming computing technologies with a focus on performance analysis, collaboration and open sharing of results. 

As a HPC center, we help researchers to solve challenges associated with their applications on our infrastructure. In this particular initiative, we focus on evaluating, assessing, and benchmarking modern and emerging computing architectures for several application domains such as high performance computing, machine Learning and data visualisation. We have identified four major dimensions of effort. 

1. Consistent, Reproducible and Open benchmarking.
2. Access to the latest compute resources for external and internal users. 
3. Management and professional administration of  experimental compute resources, with easy access and provisioning for external parties. 
4. Knowledge dissemination through workshops and the development of innovative HPC services.



### Open Benchmarking

Here the focus is on aggregrating several relevant benchmarks to standardised compilation, execution and results extraction in a common framework. The idea is to spend less time organising benchmarks and more time in designing new tests, execution and qualitative analysis of new computing systems making the process of benchmarking itself more reproducible, open and community engaging.
	
We are using [**Reframe**](https://github.com/eth-cscs/reframe), a HPC regression testing framework from [**CSCS**](https://www.cscs.ch) to automate the process of benchmarking new systems and compute architectures. We are working on developing core tests in [**Reframe**](https://github.com/eth-cscs/reframe) and would be making it open source soon. 

The tests pipeline would involve minimal software installation on the remote system and flexible integration of new tests and benchmarks inline with **SURF's** HPC infrastructure. We have been using **Reframe** to test experimental configuration located inside **SURFsara** and **University of Amsterdam** and would also be using it to test **DAS-6** systems. 

Our team here at **SURFsara** would be enthusiastic to include different benchmarks covering different scientific disciplines and domains. 
	
<Diagram to explain>

### Access to experimental compute resources

